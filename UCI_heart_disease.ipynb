{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI Heart Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Darshan Sundaram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import  KFold,GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, make_scorer, log_loss, roc_curve, confusion_matrix, f1_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start off by installing all the required packages. I will be using pandas to wrangle the two data sets. Several built in functions in sklearn and mlxtend will assist me in developing and optimizing this SVM model. I will cycle through most of the applicable kernels and their parameters to find the optimized parameter set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groundwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I load the training and test datasets, and wrangle the independent and dependent variables into their own array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>255</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>204</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>267</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>111</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>170</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>225</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>145</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     0    1    2      3      4    5    6      7    8    9   10  \\\n",
       "0         255  71.0  0.0  2.0  160.0  302.0  0.0  0.0  162.0  0.0  0.4  1.0   \n",
       "1         204  55.0  1.0  4.0  160.0  289.0  0.0  2.0  145.0  1.0  0.8  2.0   \n",
       "2         267  56.0  0.0  2.0  140.0  294.0  0.0  2.0  153.0  0.0  1.3  2.0   \n",
       "3          92  54.0  1.0  4.0  124.0  266.0  0.0  2.0  109.0  1.0  2.2  2.0   \n",
       "4         149  41.0  0.0  3.0  112.0  268.0  0.0  2.0  172.0  1.0  0.0  1.0   \n",
       "5         111  41.0  1.0  2.0  110.0  235.0  0.0  0.0  153.0  0.0  0.0  1.0   \n",
       "6         170  69.0  1.0  1.0  160.0  234.0  1.0  2.0  131.0  0.0  0.1  2.0   \n",
       "7         225  41.0  1.0  2.0  135.0  203.0  0.0  0.0  132.0  0.0  0.0  2.0   \n",
       "8         100  44.0  0.0  3.0  108.0  141.0  0.0  0.0  175.0  0.0  0.6  2.0   \n",
       "9         145  53.0  1.0  4.0  123.0  282.0  0.0  0.0   95.0  1.0  2.0  2.0   \n",
       "\n",
       "    11   12  13  \n",
       "0  2.0  3.0   1  \n",
       "1  1.0  7.0   2  \n",
       "2  0.0  3.0   1  \n",
       "3  1.0  7.0   2  \n",
       "4  0.0  3.0   1  \n",
       "5  0.0  3.0   1  \n",
       "6  1.0  3.0   1  \n",
       "7  0.0  6.0   1  \n",
       "8  0.0  3.0   1  \n",
       "9  2.0  7.0   2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = pd.read_csv('hearttrain.csv')\n",
    "dataset2 = pd.read_csv('heartval.csv') \n",
    "dataset3 = pd.read_csv('hearttest.csv')\n",
    "    \n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = dataset.iloc[:,[1,2,3,4,5,6,7,8,9,10,11,12,13]]\n",
    "train_lab = dataset.iloc[:,14]\n",
    "\n",
    "val_val = dataset2.iloc[:,[1,2,3,4,5,6,7,8,9,10,11,12,13]]\n",
    "val_lab = dataset2.iloc[:,14]\n",
    "\n",
    "test_val = dataset3.iloc[:,[1,2,3,4,5,6,7,8,9,10,11,12,13]]\n",
    "test_lab = dataset3.iloc[:,14]\n",
    "\n",
    "cv = pd.concat([dataset,dataset2])\n",
    "cv_val = cv.iloc[:,[1,2,3,4,5,6,7,8,9,10,11,12,13]]\n",
    "cv_lab = cv.iloc[:,14]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "accf = []\n",
    "specf=[]\n",
    "sensf=[]\n",
    "f1f = []\n",
    "\n",
    "GB = GaussianNB()\n",
    "\n",
    "GB.fit(train_val,train_lab)\n",
    "#validation\n",
    "predict = GB.predict(val_val)\n",
    "confusion = confusion_matrix(val_lab,predict)\n",
    "acc = accuracy_score(val_lab, predict)\n",
    "f1 = f1_score(val_lab,predict)\n",
    "TN, FP    = confusion[0, 0], confusion[0, 1]\n",
    "FN, TP    = confusion[1, 0], confusion[1, 1]\n",
    "spec = round(TN / float(TN + FP),3)*100\n",
    "sens = round(TP / float(TP + FN),3)*100\n",
    "accf.append(acc)\n",
    "f1f.append(f1)\n",
    "specf.append(spec)\n",
    "sensf.append(sens)\n",
    "#training\n",
    "predict1 = GB.predict(train_val)\n",
    "confusion1 = confusion_matrix(train_lab,predict1)\n",
    "acc1 = accuracy_score(train_lab, predict1)\n",
    "f11 = f1_score(train_lab,predict1)\n",
    "TN1, FP1   = confusion1[0, 0], confusion1[0, 1]\n",
    "FN1, TP1    = confusion1[1, 0], confusion1[1, 1]\n",
    "spec1 = round(TN1 / float(TN1 + FP1),3)*100\n",
    "sens1 = round(TP1 / float(TP1 + FN1),3)*100\n",
    "accf.append(acc1)\n",
    "f1f.append(f11)\n",
    "specf.append(spec1)\n",
    "sensf.append(sens1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>66.7</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training</th>\n",
       "      <td>0.872093</td>\n",
       "      <td>0.887755</td>\n",
       "      <td>89.7</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            accuracy        F1  Specificity  Sensitivity\n",
       "Validation  0.750000  0.744186         66.7         85.0\n",
       "Training    0.872093  0.887755         89.7         84.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = {'accuracy': accf, 'F1': f1f, \"Specificity\": specf, \"Sensitivity\": sensf}\n",
    "pd.DataFrame(dic, index = ['Validation', 'Training'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of predicting heart disease, in my opinion, the most important of the parameters at hand are F1 and Sensitivity. Although accuracy and specificity as still valuable, they are less so in this case. Accuracy for one, can be affected by imbalanced datasets whereas F1 offers a harmonic mean of precision and recall. Sensitivity is the ability to identify individuals who has the heart disease accuractely. Specificity is the ability to identify individuals who dont have the heart disease accurately. In cases like medical diagnosis, error in diagnosis, where cases of heart disease are incorrectly labelled as no heart disease can be very damaging. The capture of sensitivity, controls for false negatives. Henceforth, F1 and sensitivity will be the two metrics I look for first. \n",
    "\n",
    "Here, using a Gaussian Naive Bayes classifier (great for binary problems), we get an F1 score of 74.4% and a Sensitivity of 85%. The accuracy is 75% and specificity is 66.7%. We have a respectable F1 score, and capture a fairly high percent of our positives. The training values will be higher across the board, as we trained our model on the training data, our model will perform better on it, as we see. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will first try to simply train on the training set and predict on the validation set. Using those prediction I will find the required scores. Additionally I will be using Robust Scaler to scale my data as after visual inspection I found a few outliers within the feature set, however any other scaler (MinMax/ StandardScaler) could also have been used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_1 = []\n",
    "f1_1 = []\n",
    "sens_1 = []\n",
    "spec_1 = []\n",
    "for al in (1e-17,0.001, 0.01,0.1,1):\n",
    "    model1 = [(\"Robust\",RobustScaler()),('SVM',SGDClassifier(loss = 'log',penalty = \"l2\",alpha = al))]\n",
    "    model = Pipeline(model1)\n",
    "    model.fit(train_val,train_lab)\n",
    "    predict = model.predict(val_val)\n",
    "    confusion = confusion_matrix(val_lab,predict)\n",
    "    acc = accuracy_score(val_lab, predict)\n",
    "    f1 = f1_score(val_lab,predict)\n",
    "    TN, FP    = confusion[0, 0], confusion[0, 1]\n",
    "    FN, TP    = confusion[1, 0], confusion[1, 1]\n",
    "    spec = round(TN / float(TN + FP),3)*100\n",
    "    sens = round(TP / float(TP + FN),3)*100\n",
    "    accuracy_1.append(acc)\n",
    "    f1_1.append(f1)\n",
    "    spec_1.append(spec)\n",
    "    sens_1.append(sens)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1e-17</th>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>83.3</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001</th>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>58.3</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>79.2</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>83.3</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>83.3</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       accuracy        F1  Specificity  Sensitivity\n",
       "1e-17  0.772727  0.800000         83.3         70.0\n",
       "0.001  0.659091  0.651163         58.3         75.0\n",
       "0.01   0.795455  0.808511         79.2         80.0\n",
       "0.1    0.795455  0.816327         83.3         75.0\n",
       "1      0.772727  0.800000         83.3         70.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic1 = {'accuracy': accuracy_1, 'F1': f1_1, \"Specificity\": spec_1, \"Sensitivity\": sens_1}\n",
    "pd.DataFrame(dic1, index = ['1e-17','0.001','0.01','0.1','1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not happy with the reliability of these measures and I want a bigger dataset to find my scores on, as a result I have combined my training and validation set into a cross validation dataset and will perform 5 fold CV next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity(y_true, y_pred): #custom scorers\n",
    "    confusion = confusion_matrix(y_true,y_pred)\n",
    "    TN, FP    = confusion[0, 0], confusion[0, 1]\n",
    "    FN, TP    = confusion[1, 0], confusion[1, 1]\n",
    "    spec = round(TN / float(TN + FP),3)*100\n",
    "    return spec\n",
    "def sensitivity(y_true, y_pred):\n",
    "    confusion = confusion_matrix(y_true,y_pred)\n",
    "    TN, FP    = confusion[0, 0], confusion[0, 1]\n",
    "    FN, TP    = confusion[1, 0], confusion[1, 1]\n",
    "    sens = round(TP / float(TP + FN),3)*100\n",
    "    return sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = [(\"Standard\",RobustScaler()),\n",
    "              ('SGD',SGDClassifier(loss = 'log',penalty = \"l2\", random_state =45))]\n",
    "model = Pipeline(model1)\n",
    "scoring = {'F1': 'f1', 'Accuracy': 'accuracy', 'Sensitivity': make_scorer(sensitivity), 'Specificity': make_scorer(specificity)}\n",
    "param_grid1 = [{ \"SGD__alpha\":[1,0.1,0.01,0.001,1e-17]}]  #Parameter grid\n",
    "k_fold = KFold(5, shuffle= True, random_state = 9)\n",
    "grid = GridSearchCV(model,param_grid = param_grid1,cv = k_fold,scoring = scoring,verbose = 3, n_jobs = -1, refit = 'Accuracy') #GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  25 | elapsed:    6.2s remaining:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  25 | elapsed:    6.2s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    6.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=9, shuffle=True),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('Standard', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
       "       with_scaling=True)), ('SGD', SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate=...dom_state=45, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid=[{'SGD__alpha': [1, 0.1, 0.01, 0.001, 1e-17]}],\n",
       "       pre_dispatch='2*n_jobs', refit='Accuracy',\n",
       "       return_train_score='warn',\n",
       "       scoring={'F1': 'f1', 'Accuracy': 'accuracy', 'Sensitivity': make_scorer(sensitivity), 'Specificity': make_scorer(specificity)},\n",
       "       verbose=3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(cv_val,cv_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.03011937, 0.0341084 , 0.01835165, 0.03231382, 0.01715541]),\n",
       " 'std_fit_time': array([0.0174163 , 0.01251023, 0.00406832, 0.01224114, 0.00486166]),\n",
       " 'mean_score_time': array([0.0069819 , 0.00638518, 0.00858006, 0.00758023, 0.00758018]),\n",
       " 'std_score_time': array([0.00189203, 0.00101672, 0.00286784, 0.00119764, 0.00348921]),\n",
       " 'param_SGD__alpha': masked_array(data=[1, 0.1, 0.01, 0.001, 1e-17],\n",
       "              mask=[False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'SGD__alpha': 1},\n",
       "  {'SGD__alpha': 0.1},\n",
       "  {'SGD__alpha': 0.01},\n",
       "  {'SGD__alpha': 0.001},\n",
       "  {'SGD__alpha': 1e-17}],\n",
       " 'split0_test_F1': array([0.84      , 0.83333333, 0.88888889, 0.90909091, 0.8       ]),\n",
       " 'split1_test_F1': array([0.84745763, 0.85245902, 0.86666667, 0.87719298, 0.86206897]),\n",
       " 'split2_test_F1': array([0.88      , 0.93877551, 0.93877551, 0.85714286, 0.82758621]),\n",
       " 'split3_test_F1': array([0.7027027 , 0.72222222, 0.76470588, 0.68292683, 0.73684211]),\n",
       " 'split4_test_F1': array([0.92537313, 0.88888889, 0.86666667, 0.86666667, 0.84745763]),\n",
       " 'mean_test_F1': array([0.83911083, 0.84707189, 0.86525067, 0.83893038, 0.8147225 ]),\n",
       " 'std_test_F1': array([0.07438024, 0.07189936, 0.05659758, 0.07973862, 0.04409237]),\n",
       " 'rank_test_F1': array([3, 2, 1, 4, 5]),\n",
       " 'split0_train_F1': array([0.85185185, 0.86384977, 0.87128713, 0.83673469, 0.76571429]),\n",
       " 'split1_train_F1': array([0.86010363, 0.86734694, 0.86597938, 0.86868687, 0.84816754]),\n",
       " 'split2_train_F1': array([0.85024155, 0.85714286, 0.85279188, 0.83902439, 0.83870968]),\n",
       " 'split3_train_F1': array([0.87068966, 0.88      , 0.88888889, 0.85833333, 0.8583691 ]),\n",
       " 'split4_train_F1': array([0.82524272, 0.85714286, 0.84615385, 0.80898876, 0.81142857]),\n",
       " 'mean_train_F1': array([0.85162588, 0.86509648, 0.86502022, 0.84235361, 0.82447783]),\n",
       " 'std_train_F1': array([0.01505549, 0.00842916, 0.01493573, 0.02051663, 0.03327402]),\n",
       " 'split0_test_Accuracy': array([0.81818182, 0.81818182, 0.88636364, 0.90909091, 0.81818182]),\n",
       " 'split1_test_Accuracy': array([0.79069767, 0.79069767, 0.81395349, 0.8372093 , 0.81395349]),\n",
       " 'split2_test_Accuracy': array([0.86046512, 0.93023256, 0.93023256, 0.8372093 , 0.76744186]),\n",
       " 'split3_test_Accuracy': array([0.74418605, 0.76744186, 0.81395349, 0.69767442, 0.76744186]),\n",
       " 'split4_test_Accuracy': array([0.88372093, 0.8372093 , 0.81395349, 0.81395349, 0.79069767]),\n",
       " 'mean_test_Accuracy': array([0.81944444, 0.8287037 , 0.85185185, 0.81944444, 0.79166667]),\n",
       " 'std_test_Accuracy': array([0.0494955 , 0.05589019, 0.04820191, 0.06870644, 0.02181586]),\n",
       " 'rank_test_Accuracy': array([3, 2, 1, 3, 5]),\n",
       " 'split0_train_Accuracy': array([0.81395349, 0.83139535, 0.84883721, 0.81395349, 0.76162791]),\n",
       " 'split1_train_Accuracy': array([0.84393064, 0.84971098, 0.84971098, 0.84971098, 0.83236994]),\n",
       " 'split2_train_Accuracy': array([0.82080925, 0.83236994, 0.83236994, 0.80924855, 0.79768786]),\n",
       " 'split3_train_Accuracy': array([0.8265896 , 0.84393064, 0.85549133, 0.80346821, 0.80924855]),\n",
       " 'split4_train_Accuracy': array([0.79190751, 0.84393064, 0.83815029, 0.80346821, 0.80924855]),\n",
       " 'mean_train_Accuracy': array([0.8194381 , 0.84026751, 0.84491195, 0.81596989, 0.80203656]),\n",
       " 'std_train_Accuracy': array([0.016976  , 0.00717082, 0.00840695, 0.01732217, 0.023134  ]),\n",
       " 'split0_test_Sensitivity': array([68.2, 72.7, 86.4, 90.9, 90.9]),\n",
       " 'split1_test_Sensitivity': array([60. , 53.3, 60. , 73.3, 66.7]),\n",
       " 'split2_test_Sensitivity': array([78.9, 89.5, 89.5, 78.9, 47.4]),\n",
       " 'split3_test_Sensitivity': array([70.4, 74.1, 81.5, 59.3, 70.4]),\n",
       " 'split4_test_Sensitivity': array([58.3, 66.7, 75. , 75. , 75. ]),\n",
       " 'mean_test_Sensitivity': array([67.16481481, 71.26666667, 78.51666667, 75.55138889, 70.17638889]),\n",
       " 'std_test_Sensitivity': array([ 7.45525933, 11.68813897, 10.44817014, 10.19153143, 14.06429508]),\n",
       " 'rank_test_Sensitivity': array([5, 3, 1, 2, 4]),\n",
       " 'split0_train_Sensitivity': array([65.8, 69.9, 79.5, 79.5, 87.7]),\n",
       " 'split1_train_Sensitivity': array([78.8, 77.5, 78.8, 76.2, 78.8]),\n",
       " 'split2_train_Sensitivity': array([71.1, 75. , 78.9, 71.1, 61.8]),\n",
       " 'split3_train_Sensitivity': array([61.8, 69.1, 70.6, 52.9, 58.8]),\n",
       " 'split4_train_Sensitivity': array([62.7, 78.3, 81.9, 80.7, 83.1]),\n",
       " 'mean_train_Sensitivity': array([68.04, 73.96, 77.94, 72.08, 74.04]),\n",
       " 'std_train_Sensitivity': array([ 6.28668434,  3.8092519 ,  3.83802032, 10.15054678, 11.60527466]),\n",
       " 'split0_test_Specificity': array([95.5, 90.9, 90.9, 90.9, 72.7]),\n",
       " 'split1_test_Specificity': array([89.3, 92.9, 92.9, 89.3, 89.3]),\n",
       " 'split2_test_Specificity': array([ 91.7,  95.8,  95.8,  87.5, 100. ]),\n",
       " 'split3_test_Specificity': array([81.2, 81.2, 81.2, 87.5, 87.5]),\n",
       " 'split4_test_Specificity': array([100. ,  90.3,  83.9,  83.9,  80.6]),\n",
       " 'mean_test_Specificity': array([91.55833333, 90.22314815, 88.94907407, 87.83425926, 85.95833333]),\n",
       " 'std_test_Specificity': array([6.30319033, 4.89085705, 5.50049653, 2.3383938 , 9.13623357]),\n",
       " 'rank_test_Specificity': array([1, 2, 3, 4, 5]),\n",
       " 'split0_train_Specificity': array([92.9, 92.9, 88.9, 82.8, 67.7]),\n",
       " 'split1_train_Specificity': array([89.2, 91.4, 90.3, 92.5, 87.1]),\n",
       " 'split2_train_Specificity': array([90.7, 89.7, 86.6, 88.7, 93.8]),\n",
       " 'split3_train_Specificity': array([96.2, 94.3, 95.2, 98.1, 95.2]),\n",
       " 'split4_train_Specificity': array([94.4, 90. , 85.6, 80. , 78.9]),\n",
       " 'mean_train_Specificity': array([92.68, 91.66, 89.32, 88.42, 84.54]),\n",
       " 'std_train_Specificity': array([ 2.50710989,  1.74195293,  3.37484814,  6.52545784, 10.21109201])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I see that we have a lot of output there. I will only focus on the testing set mean scores and will be save them as a dictionary so I can access them as a data frame for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_F1</th>\n",
       "      <th>std_test_F1</th>\n",
       "      <th>mean_test_Accuracy</th>\n",
       "      <th>std_test_Accuracy</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>std_test_Sensitivity</th>\n",
       "      <th>mean_test_Specificity</th>\n",
       "      <th>std_test_Specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.839111</td>\n",
       "      <td>0.074380</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.049495</td>\n",
       "      <td>67.164815</td>\n",
       "      <td>7.455259</td>\n",
       "      <td>91.558333</td>\n",
       "      <td>6.303190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.847072</td>\n",
       "      <td>0.071899</td>\n",
       "      <td>0.828704</td>\n",
       "      <td>0.055890</td>\n",
       "      <td>71.266667</td>\n",
       "      <td>11.688139</td>\n",
       "      <td>90.223148</td>\n",
       "      <td>4.890857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <td>0.865251</td>\n",
       "      <td>0.056598</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.048202</td>\n",
       "      <td>78.516667</td>\n",
       "      <td>10.448170</td>\n",
       "      <td>88.949074</td>\n",
       "      <td>5.500497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001</th>\n",
       "      <td>0.838930</td>\n",
       "      <td>0.079739</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.068706</td>\n",
       "      <td>75.551389</td>\n",
       "      <td>10.191531</td>\n",
       "      <td>87.834259</td>\n",
       "      <td>2.338394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1e-17</th>\n",
       "      <td>0.814723</td>\n",
       "      <td>0.044092</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.021816</td>\n",
       "      <td>70.176389</td>\n",
       "      <td>14.064295</td>\n",
       "      <td>85.958333</td>\n",
       "      <td>9.136234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean_test_F1  std_test_F1  mean_test_Accuracy  std_test_Accuracy  \\\n",
       "1          0.839111     0.074380            0.819444           0.049495   \n",
       "0.1        0.847072     0.071899            0.828704           0.055890   \n",
       "0.01       0.865251     0.056598            0.851852           0.048202   \n",
       "0.001      0.838930     0.079739            0.819444           0.068706   \n",
       "1e-17      0.814723     0.044092            0.791667           0.021816   \n",
       "\n",
       "       mean_test_Sensitivity  std_test_Sensitivity  mean_test_Specificity  \\\n",
       "1                  67.164815              7.455259              91.558333   \n",
       "0.1                71.266667             11.688139              90.223148   \n",
       "0.01               78.516667             10.448170              88.949074   \n",
       "0.001              75.551389             10.191531              87.834259   \n",
       "1e-17              70.176389             14.064295              85.958333   \n",
       "\n",
       "       std_test_Specificity  \n",
       "1                  6.303190  \n",
       "0.1                4.890857  \n",
       "0.01               5.500497  \n",
       "0.001              2.338394  \n",
       "1e-17              9.136234  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic3 = {'mean_test_F1': ([0.83911083, 0.84707189, 0.86525067, 0.83893038, 0.8147225 ]),\n",
    " 'std_test_F1': ([0.07438024, 0.07189936, 0.05659758, 0.07973862, 0.04409237]),\n",
    "        'mean_test_Accuracy': ([0.81944444, 0.8287037 , 0.85185185, 0.81944444, 0.79166667]),\n",
    " 'std_test_Accuracy': ([0.0494955 , 0.05589019, 0.04820191, 0.06870644, 0.02181586]),\n",
    "\n",
    "'mean_test_Sensitivity': ([67.16481481, 71.26666667, 78.51666667, 75.55138889, 70.17638889]),\n",
    " 'std_test_Sensitivity': ([ 7.45525933, 11.68813897, 10.44817014, 10.19153143, 14.06429508]),\n",
    "\n",
    "\n",
    " 'mean_test_Specificity': ([91.55833333, 90.22314815, 88.94907407, 87.83425926, 85.95833333]),\n",
    " 'std_test_Specificity': ([6.30319033, 4.89085705, 5.50049653, 2.3383938 , 9.13623357])}\n",
    "pd.DataFrame(dic3, index = ['1','0.1','0.01','0.001','1e-17'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I combine the validation and the training set to create a cross-validation dataset. I use gridsearchCV to run through the different regularization parameters, and use the required scoring metric (two customized for sensitivity/specificity). As we see, we get the highest F1 score (86.5%) and the sensitivity of 78.5%. The specificity is 3rd highest (88.84% compared to 91.55% and 90.223% for alpha = 1 and 0.1 respectively). This change is marginally better and only ranks 3rd on my priority list of metrics to consider. The accuracy is also the highest for an alpha of 0.01. As a result, my ideal alpha value is 0.01. However a combination of random_state usage and scaler influence can change this alpha value. Ideally I want to have a much bigger dataset so these conflicts or subjectivity doesn't occur. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error graphs for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "b = []\n",
    "\n",
    "for i in range(0,1000,1):\n",
    "    model1 = [(\"Robust\",RobustScaler()),\n",
    "              ('SVM',SGDClassifier(loss = 'log',alpha = 0.01,penalty = \"l2\", n_iter = i, random_state = 45))]\n",
    "    model = Pipeline(model1)\n",
    "    #model = SGDClassifier(loss = 'log',alpha = 0.001,penalty = \"l2\", n_iter = i,shuffle = True,random_state = 7)\n",
    "    model.fit(train_val,train_lab)\n",
    "    trainp = model.predict_proba(train_val)\n",
    "    valp = model.predict_proba(val_val)\n",
    "    a.append(log_loss(train_lab,trainp))\n",
    "    b.append(log_loss(val_lab,valp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {}\n",
    "dict[2] = \"2edf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: '2edf'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.index(min(b)) #index of lowest log-loss at point of overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Val and Training log-loss minimization')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(0,1000,1),a, label = \"Training log-loss\")\n",
    "plt.plot(range(0,1000,1),b, label = \"Validation log-loss\")\n",
    "plt.legend(loc = 'best')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Val and Training log-loss\")\n",
    "plt.title(\"Val and Training log-loss minimization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using stochastic gradient descent, I came to a conclusion that the optimal alpha value is 0.01. Using that alpha value, I perform optimization through an iterative fashion. Using those results, I came to the conclusion that 3 iterations is the optimal number passes over the training data, as the log loss was minized at that pass. Using this combined model I would then use it to predict the final model. Comparing this to the Naive Bayes classifier, we have higher F1 score (86.5% VS 74.4%) and a lower sensitivity (78.5% vs 85.5%). The difference in sensitivity is quite big however I will stick to the Logistic regression model as the values are the result of  5 fold CV on a large training + validation dataset. The Naive Bayes has more variance to it as it was a one-off classification. Additionally we have a much higher specificity (88.94% vs 66.7%) and accuracy (86.5% vs 75%). The favor of 3 metrics towards the Logistic regression with SGD is enough to make me choose that as my ideal model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>SGD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.865251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.851852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>66.700000</td>\n",
       "      <td>88.949074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>85.000000</td>\n",
       "      <td>78.516667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Naive Bayes        SGD\n",
       "Accuracy        0.750000   0.865251\n",
       "F1              0.744186   0.851852\n",
       "Specificity    66.700000  88.949074\n",
       "Sensitivity    85.000000  78.516667"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_final = {\"Naive Bayes\": (0.750000,0.744186,66.7,85.0), \"SGD\": (0.865251,0.85185185,88.94907407,78.51666667)}\n",
    "pd.DataFrame(dict_final, index = ['Accuracy','F1','Specificity','Sensitivity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Testing</th>\n",
       "      <td>0.907407</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>93.1</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy        F1  Specificity  Sensitivity\n",
       "Testing  0.907407  0.915254         93.1         88.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = [(\"Robust\",StandardScaler()),\n",
    "              ('SVM',SGDClassifier(loss = 'log',alpha = 0.1,penalty = \"l2\", n_iter = 3, random_state = 45))]\n",
    "model = Pipeline(model1)\n",
    "#model = SGDClassifier(loss = 'log',alpha = 0.001,penalty = \"l2\", n_iter = i,shuffle = True,random_state = 7)\n",
    "model.fit(train_val,train_lab)\n",
    "predict = model.predict(test_val)\n",
    "confusion = confusion_matrix(test_lab,predict)\n",
    "acc5 = accuracy_score(test_lab, predict)\n",
    "f15 = f1_score(test_lab,predict)\n",
    "TN, FP    = confusion[0, 0], confusion[0, 1]\n",
    "FN, TP    = confusion[1, 0], confusion[1, 1]\n",
    "spec5 = round(TN / float(TN + FP),3)*100\n",
    "sens5 = round(TP / float(TP + FN),3)*100\n",
    "dic = {'accuracy': acc5, 'F1': f15, \"Specificity\": spec5, \"Sensitivity\": sens5}\n",
    "pd.DataFrame(dic, index = ['Testing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I performed a test after predicting using the test dataset features and we get an F1 score of 91.5%, 88% sensitivity, 93.1% specificity and 90.7% accuracy. Pretty good values! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output predicted test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>Predicted Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>134</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>49</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>61</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>15</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>162</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>43</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>121</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>133</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>201</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>171</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0     0    1    2      3      4    5    6      7    8    9   10  \\\n",
       "44         134  54.0  1.0  3.0  150.0  232.0  0.0  2.0  165.0  0.0  1.6  1.0   \n",
       "45          49  65.0  0.0  4.0  150.0  225.0  0.0  2.0  114.0  0.0  1.0  2.0   \n",
       "46          61  51.0  0.0  4.0  130.0  305.0  0.0  0.0  142.0  1.0  1.2  2.0   \n",
       "47          15  71.0  0.0  4.0  112.0  149.0  0.0  0.0  125.0  0.0  1.6  2.0   \n",
       "48         162  55.0  1.0  2.0  130.0  262.0  0.0  0.0  155.0  0.0  0.0  1.0   \n",
       "49          43  46.0  1.0  2.0  101.0  197.0  1.0  0.0  156.0  0.0  0.0  1.0   \n",
       "50         121  54.0  1.0  4.0  122.0  286.0  0.0  2.0  116.0  1.0  3.2  2.0   \n",
       "51         133  64.0  1.0  4.0  120.0  246.0  0.0  2.0   96.0  1.0  2.2  3.0   \n",
       "52         201  58.0  1.0  4.0  125.0  300.0  0.0  2.0  171.0  0.0  0.0  1.0   \n",
       "53         171  69.0  1.0  3.0  140.0  254.0  0.0  2.0  146.0  0.0  2.0  2.0   \n",
       "\n",
       "     11   12  13  Predicted Label  \n",
       "44  0.0  7.0   1                2  \n",
       "45  3.0  7.0   2                2  \n",
       "46  0.0  7.0   2                2  \n",
       "47  0.0  3.0   1                1  \n",
       "48  0.0  3.0   1                1  \n",
       "49  0.0  7.0   1                1  \n",
       "50  2.0  3.0   2                2  \n",
       "51  1.0  3.0   2                2  \n",
       "52  2.0  7.0   2                2  \n",
       "53  3.0  7.0   2                2  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3[\"Predicted Label\"] = predict #Adding 'Label' column\n",
    "\n",
    "xport = dataset3.to_csv('hw1_testset_out_final.csv') #Exporting to .csv\n",
    "dataset3.tail(10)  #sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
